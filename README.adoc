= Lucidworks Spark/Solr Integration
:toc:
:toc-placement!:

This project includes tools for reading data from Solr as a Spark RDD and indexing objects from Spark into Solr using SolrJ.

toc::[]

== Getting started

=== Import jar file via spark-shell

[source]
cd $SPARK_HOME
./bin/spark-shell --packages "com.lucidworks.solr:spark-solr:2.0.0"

=== Connect to your SolrCloud instance

==== via DataFrame

[source]
val options = Map(
  "collection" -> "{solr_collection_name}",
  "zkhost" -> "{zk_connect_string}"
)
val df = sqlContext.read.format("solr")
  .options(options)
  .load

==== via RDD

[source]
import com.lucidworks.spark.rdd.SolrRDD
val solrRDD = new SolrRDD(zkHost, collectionName, sc)

SolrRDD is an RDD of https://lucene.apache.org/solr/5_4_1/solr-solrj/org/apache/solr/common/SolrDocument.html[SolrDocument]

==== via RDD (Java)

[source]
SolrJavaRDD solrRDD = SolrJavaRDD.get(zkHost, collection, jsc.sc());
JavaRDD<SolrDocument> resultsRDD = solrRDD.queryShards(solrQuery);

== Download/Build the jar files

=== Maven Central

The released jar files (1.1.2, 2.0.0) can be downloaded from http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.lucidworks.spark%22%20AND%20a%3A%22spark-solr%22[maven central]. Maven central also holds the shaded, sources, javadoc for each release.

[source]
<dependency>
    <groupId>com.lucidworks.spark</groupId>
    <artifactId>spark-solr</artifactId>
    <version>2.0.0</version>
</dependency>

==== Snapshots

spark-solr snapshots are built for every commit on master branch. The snapshots can be accessed from https://oss.sonatype.org/content/repositories/snapshots/com/lucidworks/spark/spark-solr/[OSS Sonatype]

=== Build from source

[source]
mvn clean package -DskipTests

This will build 2 jars in the `target` directory:

* `spark-solr-${VERSION}.jar`
* `spark-solr-${VERSION}-shaded.jar`

(where `${VERSION}` will be something like 2.1.0-SNAPSHOT, for development builds)
The first is what you'd want to use if you were using spark-solr in your own project. The second is what you'd use to submit one of the included example apps to Spark.

== Features

* Send objects from a Spark Streaming application into Solr. The schema will be created using Schema API.
* Read the results from a Solr query as a Spark RDD or DataFrame
* Stream documents from Solr using /export handler. (Only works for exporting fields that have docValues enabled0
* Read large result sets from Solr using cursors or with export handler
* Data locality. If Spark workers and Solr processes are co-located on the same nodes, the partitions are placed on the nodes where the replicas are located

== Examples

`localhost:9983` will be used as zkhost in these examples. Instead of the main jar file, the shaded artifact should be used for these examples.

Once the shaded artifact is downloaded or built, it can be imported to the spark-shell by using the `--jars` config

[source]
./bin/spark-shell --jars spark-solr-2.1.0-SNAPSHOT-shaded.jar


=== Writing data to Solr

* Create a collection in Solr to index data to.

  Example: The below HTTP call creates a Solr collection with the name 'test-spark-solr'
  curl -X GET "http://localhost:8983/solr/admin/collections?action=create&name=test-spark-solr&collection.configName=techproducts&numShards=2&maxShardsPerNode=2"

* Read the csv file as a Spark DataFrame. The CSV file I have used is located https://github.com/lucidworks/spark-solr/blob/master/src/test/resources/test-data/nyc_yellow_taxi_sample_1k.csv[here]

[source,scala]
val csvFileLocation = "src/test/resources/test-data/nyc_yellow_taxi_sample_1k.csv"
var csvDF = sqlContext.read.format("com.databricks.spark.csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load(csvFileLocation)

* Clean up the data and create `pickup_latitude`, `dropoff_latitude` fields

[source,scala]
--------------------------------------------------------------------------------------------
// Filter out invalid lat/lon cols
csvDF = csvDF.filter("pickup_latitude >= -90 AND pickup_latitude <= 90 AND pickup_longitude >= -180 AND pickup_longitude <= 180")
csvDF = csvDF.filter("dropoff_latitude >= -90 AND dropoff_latitude <= 90 AND dropoff_longitude >= -180 AND dropoff_longitude <= 180")

// concat the lat/lon cols into a single value expected by solr location fields
csvDF = csvDF.withColumn("pickup", concat_ws(",", col("pickup_latitude"),col("pickup_longitude"))).drop("pickup_latitude").drop("pickup_longitude")
csvDF = csvDF.withColumn("dropoff", concat_ws(",", col("dropoff_latitude"),col("dropoff_longitude"))).drop("dropoff_latitude").drop("dropoff_longitude")
--------------------------------------------------------------------------------------------

* Write data to Solr. Before writing data to Solr, spark-solr tries to create the fields that exist in the csvDF but not in Solr via Schema API. For schema API to be usable in Solr, the https://cwiki.apache.org/confluence/display/solr/Schema+Factory+Definition+in+SolrConfig[ManagedIndexSchemaFactory] should be enabled. If you do not want to enable managed schema, then please manually create all the fields in the csv file in Solr

[source,scala]
----------------------------------------------------------------------------------------------
val options = Map(
  "zkhost" -> "localhost:9983",
  "collection" -> "test-spark-solr",
  "gen_uniq_key" -> "true" // Generate unique key if the 'id' field does not exist
)

// Write to Solr
csvDF.write.format("solr").options(options).mode(org.apache.spark.sql.SaveMode.Overwrite).save
----------------------------------------------------------------------------------------------

* The 999 documents should appear in Solr. If the docs are not yet visible, then an explicit commit should take place.

=== Reading data from Solr

In this section, we will try to read the csv data that is indexed to the Solr collection `test-spark-solr`

* Load the solr collection as a DataFrame

[source,scala]
--------------
val options = Map(
  "zkHost" -> "localhost:9983",
  "collection" -> "test-spark-solr"
)

val df = sqlContext.read.format("solr").options(options).load
--------------

* Every DataFrame has a schema. You can use the `printSchema()` function to get information about the fields available for the tweets DataFrame

[source,scala]
scala> df.printSchema()
root
 |-- improvement_surcharge: double (nullable = true)
 |-- _indexed_at_tdt: timestamp (nullable = true)
 |-- vendor_id: long (nullable = true)
 |-- trip_distance: double (nullable = true)
 |-- tolls_amount: double (nullable = true)
 |-- tip_amount: double (nullable = true)
 |-- id: string (nullable = false)
 |-- pickup: string (nullable = true)
 |-- payment_type: long (nullable = true)
 |-- fare_amount: double (nullable = true)
 |-- passenger_count: long (nullable = true)
 |-- dropoff: string (nullable = true)
 |-- store_and_fwd_flag: string (nullable = true)
 |-- extra: double (nullable = true)
 |-- dropoff_datetime: timestamp (nullable = true)
 |-- _version_: long (nullable = true)
 |-- rate_code_id: long (nullable = true)
 |-- total_amount: double (nullable = true)
 |-- pickup_datetime: timestamp (nullable = true)
 |-- mta_tax: double (nullable = true)



== Configuration and Tuning


== Developing a Spark Application

== Authenticating with Kerberized Solr

== Frequently Asked Questions