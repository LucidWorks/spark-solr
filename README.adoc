= Lucidworks Spark/Solr Integration
:toc:
:toclevels: 4
:toc-placement!:
:toc-title:

This project includes tools for reading data from Solr as a Spark RDD and indexing objects from Spark into Solr using SolrJ.

toc::[]

== Getting started

=== Import jar file via spark-shell

[source]
cd $SPARK_HOME
./bin/spark-shell --packages "com.lucidworks.solr:spark-solr:2.0.0"

=== Connect to your SolrCloud instance

==== via DataFrame

[source]
val options = Map(
  "collection" -> "{solr_collection_name}",
  "zkhost" -> "{zk_connect_string}"
)
val df = sqlContext.read.format("solr")
  .options(options)
  .load

==== via RDD

[source]
import com.lucidworks.spark.rdd.SolrRDD
val solrRDD = new SolrRDD(zkHost, collectionName, sc)

SolrRDD is an RDD of https://lucene.apache.org/solr/5_4_1/solr-solrj/org/apache/solr/common/SolrDocument.html[SolrDocument]

==== via RDD (Java)

[source]
SolrJavaRDD solrRDD = SolrJavaRDD.get(zkHost, collection, jsc.sc());
JavaRDD<SolrDocument> resultsRDD = solrRDD.queryShards(solrQuery);

== Download/Build the jar files

=== Maven Central

The released jar files (1.1.2, 2.0.0) can be downloaded from http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.lucidworks.spark%22%20AND%20a%3A%22spark-solr%22[maven central]. Maven central also holds the shaded, sources, javadoc for each release.

[source]
<dependency>
    <groupId>com.lucidworks.spark</groupId>
    <artifactId>spark-solr</artifactId>
    <version>2.0.0</version>
</dependency>

==== Snapshots

spark-solr snapshots are built for every commit on master branch. The snapshots can be accessed from https://oss.sonatype.org/content/repositories/snapshots/com/lucidworks/spark/spark-solr/[OSS Sonatype]

=== Build from source

[source]
mvn clean package -DskipTests

This will build 2 jars in the `target` directory:

* `spark-solr-${VERSION}.jar`
* `spark-solr-${VERSION}-shaded.jar`

(where `${VERSION}` will be something like 2.1.0-SNAPSHOT, for development builds)
The first is what you'd want to use if you were using spark-solr in your own project. The second is what you'd use to submit one of the included example apps to Spark.

== Features

* Send objects from a Spark (Streaming or DataFrames) into Solr.
* Read the results from a Solr query as a Spark RDD or DataFrame
* Stream documents from Solr using /export handler. (Only works for exporting fields that have docValues enabled0
* Read large result sets from Solr using cursors or with export handler
* Data locality. If Spark workers and Solr processes are co-located on the same nodes, the partitions are placed on the nodes where the replicas are located

== Querying

=== Cursors

https://cwiki.apache.org/confluence/display/solr/Pagination+of+Results[Cursors] are used by default to pull documents out of Solr. By default, the number of tasks allocated will be the number of shards available for the collection.

If your Spark cluster has more available executor slots than the number of shards, then you can increase parallelism when reading from Solr by splitting each shard into sub ranges using a split field. A good candidate for the split field is the version field that is attached to every document by the shard leader during indexing. See [query.split.field] and [query.splits.per.shard]

Cursors won't work if the index changes during the query time. Constrain your query to a static index by using additional Solr params in `solr.params`

=== Streaming API (/export)

If the fields that are being queried have https://cwiki.apache.org/confluence/display/solr/DocValues[docValues] enabled, then the Streaming API can be used to pull documents from Solr in a true Streaming fashion. This method is *8-10x* faster than Cursors

== Indexing

Objects can be sent to Solr via Spark Streaming or DataFrames. The schema is inferred from the DataFrame and any fields that do not exist in Solr schema will be added via Schema API. See https://cwiki.apache.org/confluence/display/solr/Schema+Factory+Definition+in+SolrConfig[ManagedIndexSchemaFactory]
See [Indexing params] for configuration and tuning

== Examples

`localhost:9983` will be used as zkhost in these examples. Instead of the main jar file, the shaded artifact should be used for these examples.

Once the shaded artifact is downloaded or built, it can be imported to the spark-shell by using the `--jars` config

    ./bin/spark-shell --jars spark-solr-2.1.0-SNAPSHOT-shaded.jar

=== Writing data

* Create a collection in Solr to index data to.

  Example: The below HTTP call creates a Solr collection with the name 'test-spark-solr'
  curl -X GET "http://localhost:8983/solr/admin/collections?action=create&name=test-spark-solr&collection.configName=techproducts&numShards=2&maxShardsPerNode=2"

* Read the csv file as a Spark DataFrame. The CSV file I have used is located https://github.com/lucidworks/spark-solr/blob/master/src/test/resources/test-data/nyc_yellow_taxi_sample_1k.csv[here]

[source,scala]
val csvFileLocation = "src/test/resources/test-data/nyc_yellow_taxi_sample_1k.csv"
var csvDF = sqlContext.read.format("com.databricks.spark.csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load(csvFileLocation)

* Clean up the data and create `pickup_latitude`, `dropoff_latitude` fields

[source,scala]
--------------
// Filter out invalid lat/lon cols
csvDF = csvDF.filter("pickup_latitude >= -90 AND pickup_latitude <= 90 AND pickup_longitude >= -180 AND pickup_longitude <= 180")
csvDF = csvDF.filter("dropoff_latitude >= -90 AND dropoff_latitude <= 90 AND dropoff_longitude >= -180 AND dropoff_longitude <= 180")

// concat the lat/lon cols into a single value expected by solr location fields
csvDF = csvDF.withColumn("pickup", concat_ws(",", col("pickup_latitude"),col("pickup_longitude"))).drop("pickup_latitude").drop("pickup_longitude")
csvDF = csvDF.withColumn("dropoff", concat_ws(",", col("dropoff_latitude"),col("dropoff_longitude"))).drop("dropoff_latitude").drop("dropoff_longitude")
--------------

* Write data to Solr. Before writing data to Solr, spark-solr tries to create the fields that exist in the csvDF but not in Solr via Schema API. For schema API to be usable in Solr, the https://cwiki.apache.org/confluence/display/solr/Schema+Factory+Definition+in+SolrConfig[ManagedIndexSchemaFactory] should be enabled. If you do not want to enable managed schema, then please manually create all the fields in the csv file in Solr

[source,scala]
--------------
val options = Map(
  "zkhost" -> "localhost:9983",
  "collection" -> "test-spark-solr",
  "gen_uniq_key" -> "true" // Generate unique key if the 'id' field does not exist
)

// Write to Solr
csvDF.write.format("solr").options(options).mode(org.apache.spark.sql.SaveMode.Overwrite).save
--------------

* 999 documents should appear in Solr. If all the docs are not yet visible, then an explicit commit can be done via HTTP call.

=== Reading data

In this section, we will try to read the csv data that is indexed to the Solr collection `test-spark-solr`

* Load the solr collection as a DataFrame

[source,scala]
--------------
val options = Map(
  "zkHost" -> "localhost:9983",
  "collection" -> "test-spark-solr"
)

val df = sqlContext.read.format("solr").options(options).load
--------------

* Every DataFrame has a schema. You can use the `printSchema()` function to get information about the fields available for the tweets DataFrame

[source,scala]
scala> df.printSchema()
root
 |-- improvement_surcharge: double (nullable = true)
 |-- _indexed_at_tdt: timestamp (nullable = true)
 |-- vendor_id: long (nullable = true)
 |-- trip_distance: double (nullable = true)
 |-- tolls_amount: double (nullable = true)
 |-- tip_amount: double (nullable = true)
 |-- id: string (nullable = false)
 |-- pickup: string (nullable = true)
 |-- payment_type: long (nullable = true)
 |-- fare_amount: double (nullable = true)
 |-- passenger_count: long (nullable = true)
 |-- dropoff: string (nullable = true)
 |-- store_and_fwd_flag: string (nullable = true)
 |-- extra: double (nullable = true)
 |-- dropoff_datetime: timestamp (nullable = true)
 |-- _version_: long (nullable = true)
 |-- rate_code_id: long (nullable = true)
 |-- total_amount: double (nullable = true)
 |-- pickup_datetime: timestamp (nullable = true)
 |-- mta_tax: double (nullable = true)

* To be able to query with SQL syntax, we need to register this DataFrame as a table

[source,scala]
df.registerTempTable("trips")

* Fire off SQL queries

[source,scala]
--------------
// Cache the DataFrame for efficiency. See http://spark.apache.org/docs/latest/sql-programming-guide.html#caching-data-in-memory
scala>df.cache()
scala> sqlContext.sql("SELECT avg(tip_amount), avg(fare_amount) FROM trips").show()
+-----------------+-----------------+
|              _c0|              _c1|
+-----------------+-----------------+
|1.630050050050051|12.27087087087087|
+-----------------+-----------------+

scala>  sqlContext.sql("SELECT max(tip_amount), max(fare_amount) FROM trips WHERE trip_distance > 10").show()
+-----+----+
|  _c0| _c1|
+-----+----+
|16.44|83.5|
+-----+----+
--------------

== Configuration and Tuning

The Solr DataSource supports a number of optional parameters that allow you to optimize performance when reading data from Solr. The only required parameters for the DataSource are `zkhost` and `collection`.

=== Query parameters

==== query

Probably the most obvious option is to specify a Solr query that limits the rows you want to load into Spark.
For instance, if we only wanted to load documents that mention "solr", we would do:

Usage: `option("query","body_t:solr")`
Default: `\*:*`

If you don't specify the "query" option, then all rows are read using the match all documents query (`\*:*`).

=== fields

You can use the "fields" option to specify a subset of fields to retrieve for each document in your results:

Usage: `option("fields","id,author_s,favorited_b,...")`

By default, all fields for each document are pulled back from Solr.

=== rows

You can use the "rows" option to specify the number of rows to retrieve from Solr per request. Behind the scenes, the implementation uses deep paging cursors or Streaming API and response streaming, so it is usually safe to specify a large number of rows. By default, the implementation uses 1000 but if your documents are smaller, you can increase this to 10000. Using too large a value can put pressure on the Solr JVM's garbage collector.

Usage: `option("rows","10000")`
Default: 1000

=== Shard Splitting

If your Spark cluster has more available executor slots than the number of shards, then you can increase parallelism when reading from Solr by splitting each shard into sub ranges using a split field. The sub range splitting enables faster fetching from Solr by increasing the number of tasks in Solr. This should only be used if there are enough computing resources in the Spark cluster.
Shard splitting is disabled by default.

==== splits

Enable shard splitting on default field `_version_`

Usage: `option("splits", "true")`
Default: false

The above option is equivalent to `option("split_field", "\_version_")`

==== split_field

The field to split on can be changed using `split_field` option.

Usage: `option("split_field", "id")`
Default: `_version_`

==== splits_per_shard

Behind the scenes, the DataSource implementation tries to split the shard into evenly sized splits using filter queries. You can also split on a string-based keyword field but it should have sufficient variance in the values to allow for creating enough splits to be useful. In other words, if your Spark cluster can handle 10 splits per shard, but there are only 3 unique values in a keyword field, then you will only get 3 splits.

Keep in mind that this is only a hint to the split calculator and you may end up with a slightly different number of splits than what was requested.

Usage: `option("splits_per_shard", "30")`
Default: 20

=== flatten_multivalued

This option is enabled by default and flattens multi valued fields from Solr.

Usage: `option("flatten_multivalued", "false")`
Default: true

=== use_export_handler

This option is disabled by default and can be used to export results from Solr via `/export` handler which streams data out of Solr. See [Export Handler]. The `/export` handler needs fields to be explicitly specified. Please use `fields` option or specify the fields in the query.

Usage: `option("use_export_handler", "true")`
Default: true

=== Index parameters

==== soft_commit_secs

If specified, the soft commit config value will be set via SolrConfig API during indexing

Usage: `option("soft_commit_secs", "10")
Default: None

==== batch_size

This option determines the number of documents that are sent to Solr via a HTTP call during indexing. Set this option higher if the docs are small and memory is available.

Usage: `option("batch_size", "10000")
Default: 500

==== gen_uniq_key

If the documents are missing the unique key (in Solr schema), then this option will generate a unique value for each documentt before indexing to Solr.

Usage: `option("gen_uniq_key", "true")
Default: false

== Developing a Spark Application

== Authenticating with Kerberized Solr

== Frequently Asked Questions
